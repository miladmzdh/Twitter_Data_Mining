{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6315535d-d181-4142-aa9c-b0c7ff3e6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import concurrent.futures\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "533185bb-b803-4410-a315-7fd77d570766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "input_file_path = '/scratch/project_2006761/Milad_Sctraches/nut3_h3_dict.pkl'\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open(input_file_path, 'rb') as file:\n",
    "    nut3_h3_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63e2df7-8db5-41ab-aa6b-b2f1ef0b8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = '/scratch/project_2006761/Milad_Sctraches/unique_user_ids.txt'\n",
    "\n",
    "unique_user_ids = []\n",
    "with open(input_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        unique_user_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7593da86-4b2a-4db3-83a7-35049c2ad1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11735941"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc790479-b298-42b7-ba89-d6d1440008c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = '/scratch/project_2006761/Milad_Sctraches/shapefiles/hex_gdf.pkl'\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open(input_file_path, 'rb') as file:\n",
    "    hex_gdf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ac854a-ece2-4e99-bab3-a592d37f4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely convert datetime columns to UTC\n",
    "def safe_convert_to_utc(column):\n",
    "    # Attempt to convert any strings or other formats to datetime first without specifying timezone\n",
    "    column = pd.to_datetime(column, errors='coerce')\n",
    "    # If already timezone-aware, convert to UTC\n",
    "    if column.dt.tz is not None:\n",
    "        return column.dt.tz_convert('UTC')\n",
    "    else:  # If timezone-naive, localize to UTC\n",
    "        return column.dt.tz_localize('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd8113c-73c9-4b77-a499-eabe9021b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine period based on date\n",
    "def determine_period(date):\n",
    "    global time_periods\n",
    "    for period_name, (start, end) in period_dates.items():\n",
    "        if start <= date <= end:\n",
    "            return period_name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2a88e0b-e5bc-44b1-b743-f452c51069bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory path containing the period folders\n",
    "base_directory_path = '/projappl/project_2006761/Twitter_Ate/eu-parquet-moves-only-max-180-days-final'\n",
    "\n",
    "distance_threshold = 100 # KM\n",
    "\n",
    "# For Cleaning\n",
    "columns_to_keep = ['u_id', 'created_at_start', 'h3_grid_res10_start', 'NUTS_ID_start',\n",
    "                       'created_at_end', 'h3_grid_res10_end', 'NUTS_ID_end']\n",
    "\n",
    "period_dates = {\n",
    "    'before_2016': (pd.Timestamp('2000-01-01').tz_localize('UTC'), pd.Timestamp('2015-12-31').tz_localize('UTC')),\n",
    "    '2016_to_2019': (pd.Timestamp('2016-01-01').tz_localize('UTC'), pd.Timestamp('2019-12-31').tz_localize('UTC')),\n",
    "    '2020_and_beyond': (pd.Timestamp('2020-01-01').tz_localize('UTC'), pd.Timestamp('2023-12-31').tz_localize('UTC'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33846da2-3098-4acb-b892-c7e564e872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_id_of_interest = '151380704'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a962d77-0826-4058-b354-04aeee6ba112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_user(user_id_of_interest):\n",
    "    try:\n",
    "        \n",
    "        global distance_threshold, columns_to_keep, base_directory_path\n",
    "        \n",
    "        user_id_first_digit = user_id_of_interest[0]  # Get the first digit/character\n",
    "        \n",
    "        # Initialize an empty DataFrame to store data for the user of interest\n",
    "        user_data = pd.DataFrame()\n",
    "        \n",
    "        # Define periods to iterate over\n",
    "        periods = ['period=1', 'period=2']\n",
    "        \n",
    "        # Iterate only through the relevant directories based on the user ID's first digit\n",
    "        for period in periods:\n",
    "            user_digit_dir = f\"user_first_digit={user_id_first_digit}\"\n",
    "            directory_path = os.path.join(base_directory_path, period, user_digit_dir)\n",
    "            \n",
    "            # Check if the specific directory exists before attempting to read files\n",
    "            if os.path.exists(directory_path):\n",
    "                for file in os.listdir(directory_path):\n",
    "                    if file.endswith('.parquet'):\n",
    "                        file_path = os.path.join(directory_path, file)\n",
    "                        try:\n",
    "                            # Attempt to read and filter the Parquet file\n",
    "                            temp_df = pd.read_parquet(file_path, filters=[('u_id', '=', user_id_of_interest)])\n",
    "                            \n",
    "                            # Append the filtered data to the user_data DataFrame\n",
    "                            # Check if temp_df is not empty and not all NA\n",
    "                            if not temp_df.empty and not temp_df.isna().all().all():\n",
    "                                # Only then, concatenate with user_data\n",
    "                                user_data = pd.concat([user_data, temp_df], ignore_index=True)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to process {file_path}: {e}\")\n",
    "        \n",
    "        # Celan the data\n",
    "        user_data = user_data[columns_to_keep]\n",
    "        \n",
    "    \n",
    "        # Apply the conversion/localization function to your datetime columns\n",
    "        user_data['created_at_start'] = safe_convert_to_utc(user_data['created_at_start'])\n",
    "        user_data['created_at_end'] = safe_convert_to_utc(user_data['created_at_end'])\n",
    "        \n",
    "        user_trips = user_data.copy()\n",
    "        \n",
    "        user_data['d_t'] = (user_data['created_at_end'] - user_data['created_at_start']).dt.total_seconds() / (2 * 86400)\n",
    "        \n",
    "        # Create the DataFrame for the 'start' information\n",
    "        start_df = user_data[['u_id', 'created_at_start', 'h3_grid_res10_start', 'NUTS_ID_start', 'd_t']].copy()\n",
    "        # Rename the columns to have a unified naming convention\n",
    "        start_df.columns = ['u_id', 'created_at', 'h3_grid_res10', 'NUTS_ID', 'd_t']\n",
    "        \n",
    "        # Create the DataFrame for the 'end' information\n",
    "        end_df = user_data[['u_id', 'created_at_end', 'h3_grid_res10_end', 'NUTS_ID_end', 'd_t']].copy()\n",
    "        # Rename the columns to match the start_df\n",
    "        end_df.columns = ['u_id', 'created_at', 'h3_grid_res10', 'NUTS_ID', 'd_t']\n",
    "        \n",
    "        # Concatenate the two DataFrames\n",
    "        user_data = pd.concat([start_df, end_df], ignore_index=True).sort_values(by='created_at', ascending=True)\n",
    "        \n",
    "        # Group by 'created_at' and aggregate\n",
    "        user_data = user_data.groupby('created_at').agg({\n",
    "            'u_id': 'first',  # Keep the first u_id encountered\n",
    "            'h3_grid_res10': 'first',  # Keep the first h3_grid_res10 encountered\n",
    "            'NUTS_ID': 'first',  # Keep the first NUTS_ID encountered\n",
    "            'd_t': 'sum'  # Sum the d_t values\n",
    "        }).reset_index()\n",
    "    \n",
    "        # Fill the None values in NUTS_ID columns\n",
    "        # Check if there are any NaN values in the NUTS_ID column\n",
    "        if user_data['NUTS_ID'].isna().any():\n",
    "            # Map h3_grid_res10 to NUTS_ID where NUTS_ID is NaN, retaining NaN for unmatched keys in nut3_h3_dict\n",
    "            user_data.loc[user_data['NUTS_ID'].isna(), 'NUTS_ID'] = (\n",
    "                user_data.loc[user_data['NUTS_ID'].isna(), 'h3_grid_res10']\n",
    "                .map(nut3_h3_dict)\n",
    "                .fillna(user_data['NUTS_ID'])\n",
    "            )\n",
    "    \n",
    "        # Ther should not be any na, but incase\n",
    "        user_data.dropna(inplace=True)\n",
    "    \n",
    "        # Go to the next user if the data is empty\n",
    "        if user_data.empty:\n",
    "            return\n",
    "        \n",
    "        # Calculate the significant places based on frequency\n",
    "        \n",
    "        # Identify the first and last date\n",
    "        first_date = user_data['created_at'].iloc[0]\n",
    "        last_date = user_data['created_at'].iloc[-1]\n",
    "        \n",
    "        if (last_date - first_date).days > 180:\n",
    "            \n",
    "            # Identify the first and last year\n",
    "            start_year = first_date.year\n",
    "            end_year = last_date.year\n",
    "            \n",
    "            # Initialize a list to hold pairs of NUTS_ID that meet the condition\n",
    "            nuts_id_pairs = set()\n",
    "            nuts_dwell_time = set()\n",
    "    \n",
    "            # For Cross-Border\n",
    "            nuts_id_pairs_cross = set()\n",
    "            nuts_dwell_time_cross = set()     \n",
    "            \n",
    "            # Loop through the range\n",
    "            for year in range(start_year, end_year+1):\n",
    "        \n",
    "                # For the first year in the range, consider the year itself and the next two ones\n",
    "                if year == start_year:\n",
    "                    window_start = pd.to_datetime(f'{year}-01-01').tz_localize('UTC')\n",
    "                    window_end = pd.to_datetime(f'{year+2}-12-31').tz_localize('UTC')\n",
    "                # For the last year in the range, consider the year itself and the previous two ones\n",
    "                elif year == end_year:\n",
    "                    window_start = pd.to_datetime(f'{year-2}-01-01').tz_localize('UTC')\n",
    "                    window_end = pd.to_datetime(f'{year}-12-31').tz_localize('UTC')\n",
    "                # For years in between, include one year before and one after\n",
    "                else:\n",
    "                    window_start = pd.to_datetime(f'{year-1}-01-01').tz_localize('UTC')\n",
    "                    window_end = pd.to_datetime(f'{year+1}-12-31').tz_localize('UTC')\n",
    "            \n",
    "                # Use window_start and window_end to filter or perform operations on user_data\n",
    "                filtered_data = user_data[(user_data['created_at'] >= window_start) & (user_data['created_at'] <= window_end)]\n",
    "                \n",
    "                # Count the occurrences of each unique value in 'NUTS_ID_start'\n",
    "                occurrences = filtered_data['NUTS_ID'].value_counts()\n",
    "                # Calculate the 90th percentile value of the occurrences Series\n",
    "                quantile_75 = occurrences.quantile(0.75)\n",
    "                \n",
    "                # Filter the Series to include only values above the 90th percentile\n",
    "                top_quantile_h3 = occurrences[occurrences > quantile_75].index.tolist()\n",
    "                \n",
    "                # Filter the original DataFrame to include only rows with those h3_grid_res10 values\n",
    "                filtered_top_quantile_data = filtered_data[filtered_data['NUTS_ID'].isin(top_quantile_h3)]\n",
    "            \n",
    "                filtered_top_quantile_data = filtered_top_quantile_data[['h3_grid_res10', 'NUTS_ID']].drop_duplicates()\n",
    "            \n",
    "                nuts_ids = filtered_top_quantile_data['NUTS_ID'].unique()\n",
    "    \n",
    "                # Filter only those we need to optimize\n",
    "                hex_gdf_user = hex_gdf[hex_gdf['hex_id'].isin(filtered_top_quantile_data['h3_grid_res10'].unique())] \n",
    "                \n",
    "                # Check if there's more than one unique NUTS_ID before proceeding\n",
    "                if len(nuts_ids) > 1:\n",
    "                    for i, nuts_id1 in enumerate(nuts_ids):\n",
    "                        for nuts_id2 in nuts_ids[i+1:]:\n",
    "                        \n",
    "                            # Filter df for each pair of NUTS_ID\n",
    "                            df1 = filtered_top_quantile_data[filtered_top_quantile_data['NUTS_ID'] == nuts_id1]\n",
    "                            df2 = filtered_top_quantile_data[filtered_top_quantile_data['NUTS_ID'] == nuts_id2]\n",
    "                            \n",
    "                            # Calculate distances between all pairs of h3_grid_res10 in df1 and df2\n",
    "                            for h3_index1 in df1['h3_grid_res10']:\n",
    "                                for h3_index2 in df2['h3_grid_res10']:\n",
    "    \n",
    "                                    try: # In case there is a NaN value\n",
    "                                        # Calculate the distance in meters\n",
    "                                        # Extract the geometry for each hex_id directly\n",
    "                                        geom1 = hex_gdf_user.loc[hex_gdf_user['hex_id'] == h3_index1, 'geometry'].values[0]\n",
    "                                        geom2 = hex_gdf_user.loc[hex_gdf_user['hex_id'] == h3_index2, 'geometry'].values[0]\n",
    "                                        \n",
    "                                        # Calculate the distance between the two geometries\n",
    "                                        distance = geom1.distance(geom2) / 1000\n",
    "                                        \n",
    "                                        # Check if the distance exceeds the threshold\n",
    "                                        if distance > distance_threshold:\n",
    "                                            # Add the pair of NUTS_ID to the list and break the loop\n",
    "                                            nuts_id_pairs.add((year, nuts_id1, nuts_id2))\n",
    "                                            nuts_dwell_time.add((year, nuts_id1, filtered_data[filtered_data.NUTS_ID==nuts_id1].d_t.sum()))\n",
    "                                            nuts_dwell_time.add((year, nuts_id2, filtered_data[filtered_data.NUTS_ID==nuts_id2].d_t.sum()))\n",
    "                                            break\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                else:\n",
    "                                    # Continue if the inner loop wasn't broken\n",
    "                                    continue\n",
    "                                # Inner loop was broken, break the outer loop\n",
    "                                break\n",
    "\n",
    "                            if nuts_id1[:2] != nuts_id2[:2]:\n",
    "                                nuts_id_pairs_cross.add((year, nuts_id1, nuts_id2))\n",
    "                                nuts_dwell_time_cross.add((year, nuts_id1, filtered_data[filtered_data.NUTS_ID==nuts_id1].d_t.sum()))\n",
    "                                nuts_dwell_time_cross.add((year, nuts_id2, filtered_data[filtered_data.NUTS_ID==nuts_id2].d_t.sum()))\n",
    "\n",
    "                    \n",
    "            # Initialize two dictionaries to hold the values for greater or equal to 90 and less than 90\n",
    "            greater_equal_90 = {}\n",
    "            #less_than_90 = {}\n",
    "            \n",
    "            # Iterate through the list and categorize based on the third item in each tuple\n",
    "            for year, id, value in nuts_dwell_time:\n",
    "                if value >= 90:\n",
    "                    if year in greater_equal_90:\n",
    "                        greater_equal_90[year].append(id)\n",
    "                    else:\n",
    "                        greater_equal_90[year] = [id]\n",
    "                # else:\n",
    "                #     if year in less_than_90:\n",
    "                #         less_than_90[year].append(id)\n",
    "                #     else:\n",
    "                #         less_than_90[year] = [id]\n",
    "            \n",
    "            # Convert dictionaries to list of tuples\n",
    "            sig_locations = [(year, ids) for year, ids in greater_equal_90.items()]\n",
    "            #non_sig_locations = [(year, ids) for year, ids in less_than_90.items()]\n",
    "    \n",
    "            first_year = first_date.year\n",
    "            last_year = last_date.year\n",
    "            \n",
    "            # Initialize a dictionary to count occurrences of each ID within a 5-year window\n",
    "            window_counts = {}\n",
    "            \n",
    "            # Iterate through each year in the significant locations\n",
    "            for year, ids in sig_locations:\n",
    "                # Determine the window based on the year's position relative to the first and last years\n",
    "                if year == first_year:\n",
    "                    check_years = range(year, year + 5)  # First year: This year + next 4 years\n",
    "                elif year == first_year + 1:\n",
    "                    check_years = range(year - 1, year + 4)  # Second year: One year before + next 3 years\n",
    "                elif year == last_year - 1:\n",
    "                    check_years = range(year - 3, year + 2)  # Second to last year: Three years before + one year after\n",
    "                elif year == last_year:\n",
    "                    check_years = range(year - 4, year + 1)  # Last year: Four years before + this year\n",
    "                else:\n",
    "                    check_years = range(year - 2, year + 3)  # Other years: Two years before + two years after\n",
    "            \n",
    "                for check_year in check_years:\n",
    "                    if check_year in [y for y, _ in sig_locations]:  # Check if the year is within the significant locations\n",
    "                        for id in ids:\n",
    "                            # Increment the count for this ID in the adjusted window\n",
    "                            if id not in window_counts:\n",
    "                                window_counts[id] = {year: 1}\n",
    "                            else:\n",
    "                                if year not in window_counts[id]:\n",
    "                                    window_counts[id][year] = 1\n",
    "                                else:\n",
    "                                    window_counts[id][year] += 1\n",
    "    \n",
    "            # For Cross-Border commuting\n",
    "            \n",
    "            # Initialize two dictionaries to hold the values for greater or equal to 90 and less than 90\n",
    "            greater_equal_90 = {}\n",
    "            #less_than_90 = {}\n",
    "            \n",
    "            # Iterate through the list and categorize based on the third item in each tuple\n",
    "            for year, id, value in nuts_dwell_time_cross:\n",
    "                if value >= 90:\n",
    "                    if year in greater_equal_90:\n",
    "                        greater_equal_90[year].append(id)\n",
    "                    else:\n",
    "                        greater_equal_90[year] = [id]\n",
    "                # else:\n",
    "                #     if year in less_than_90:\n",
    "                #         less_than_90[year].append(id)\n",
    "                #     else:\n",
    "                #         less_than_90[year] = [id]\n",
    "            \n",
    "            # Convert dictionaries to list of tuples\n",
    "            sig_locations_cross = [(year, ids) for year, ids in greater_equal_90.items()]\n",
    "    \n",
    "            # Define the periods\n",
    "            periods = {\n",
    "                'before_2016': set(),\n",
    "                '2016_to_2019': set(),\n",
    "                '2020_and_beyond': set()\n",
    "            }\n",
    "    \n",
    "            # Merge sig_locations into three time periods\n",
    "            for year, ids in sig_locations:\n",
    "                if year < 2016:\n",
    "                    periods['before_2016'].update(ids)\n",
    "                elif 2016 <= year <= 2019:\n",
    "                    periods['2016_to_2019'].update(ids)\n",
    "                else:  # 2020 and beyond\n",
    "                    periods['2020_and_beyond'].update(ids)\n",
    "\n",
    "            \n",
    "            # Define the periods\n",
    "            periods_cross = {\n",
    "                'before_2016': set(),\n",
    "                '2016_to_2019': set(),\n",
    "                '2020_and_beyond': set()\n",
    "            }\n",
    "    \n",
    "            # Merge sig_locations into three time periods\n",
    "            for year, ids in sig_locations_cross:\n",
    "                if year < 2016:\n",
    "                    periods_cross['before_2016'].update(ids)\n",
    "                elif 2016 <= year <= 2019:\n",
    "                    periods_cross['2016_to_2019'].update(ids)\n",
    "                else:  # 2020 and beyond\n",
    "                    periods_cross['2020_and_beyond'].update(ids)\n",
    "            \n",
    "            # Get the primary location\n",
    "            # Apply function to assign period to each row in DataFrame\n",
    "            user_data['Period'] = user_data['created_at'].apply(determine_period)\n",
    "\n",
    "            # Convert nuts dwell time to a df for faster computation to get sig_locations_dict\n",
    "            df_temp = pd.DataFrame(nuts_dwell_time, columns=['Year', 'ID', 'Value'])\n",
    "            \n",
    "            # Define periods as a series of conditions and choices\n",
    "            conditions = [\n",
    "                df_temp['Year'] < 2016,\n",
    "                df_temp['Year'].between(2016, 2019),\n",
    "                df_temp['Year'] >= 2020\n",
    "            ]\n",
    "            choices = ['before_2016', '2016_to_2019', '2020_and_beyond']\n",
    "            \n",
    "            # Assign period based on year\n",
    "            df_temp['Period'] = np.select(conditions, choices)\n",
    "            \n",
    "            # Group by Period and ID, then sum values\n",
    "            grouped = df_temp.groupby(['Period', 'ID'])['Value'].sum().reset_index()\n",
    "            \n",
    "            # Find the ID with the highest value within each period\n",
    "            idx = grouped.groupby(['Period'])['Value'].idxmax()\n",
    "            highest_value_id_per_period = grouped.loc[idx]\n",
    "\n",
    "\n",
    "            # Transform into a more lookup-friendly structure\n",
    "            lookup_dict = {}\n",
    "            for year, id1, id2 in nuts_id_pairs:\n",
    "                key = frozenset([id1, id2])\n",
    "                if key in lookup_dict:\n",
    "                    lookup_dict[key].add(year)\n",
    "                else:\n",
    "                    lookup_dict[key] = {year}\n",
    "\n",
    "            # Transform into a more lookup-friendly structure for Cross Border pairs\n",
    "                lookup_dict_cross = {}\n",
    "                for year, id1, id2 in nuts_id_pairs_cross:\n",
    "                    key = frozenset([id1, id2])\n",
    "                    if key in lookup_dict_cross:\n",
    "                        lookup_dict_cross[key].add(year)\n",
    "                    else:\n",
    "                        lookup_dict_cross[key] = {year}\n",
    "            \n",
    "            '''\n",
    "            MULTILOCAL and LONG DISTANCE MOBILITY TYPE\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            # Filter IDs that have more than 2 occurrences in any 5-year window\n",
    "            ml_ld_ids = [id for id, years in window_counts.items() if any(count > 2 for count in years.values())]\n",
    "            \n",
    "            # Initialize an empty set to store the triples\n",
    "            multilocal_trips_user = set()\n",
    "\n",
    "            # Initialize the set for LD_trips_user\n",
    "            LD_trips_user = set()\n",
    "\n",
    "            # Iterate through the periods and their corresponding IDs\n",
    "            for period, ids_in_period in periods.items():\n",
    "                # Find the intersection of multilocal_ids and the current period's IDs\n",
    "                relevant_ids = set(ml_ld_ids).intersection(ids_in_period)\n",
    "                \n",
    "                # Proceed if we have more than one ID in the intersection, thus capable of forming a pair\n",
    "                if len(relevant_ids) > 1:\n",
    "\n",
    "                    home_id_series = highest_value_id_per_period.loc[highest_value_id_per_period['Period'] == period, 'ID']\n",
    "\n",
    "                    if not home_id_series.empty:\n",
    "                        home_NUTS_ID = home_id_series.iloc[0]\n",
    "    \n",
    "                        # Filter user_data\n",
    "                        filtered_data = user_data[(user_data['NUTS_ID'].isin(relevant_ids)) & (user_data['Period'] == period)]\n",
    "                        # Create an independent copy of filtered_data to avoid SettingWithCopyWarning\n",
    "                        filtered_data = filtered_data.copy()\n",
    "                        filtered_data['Prev_NUTS_ID'] = filtered_data['NUTS_ID'].shift(1)\n",
    "                        filtered_data = filtered_data[filtered_data['NUTS_ID'] != filtered_data['Prev_NUTS_ID']].drop(columns='Prev_NUTS_ID')\n",
    "\n",
    "                        if len(filtered_data) > 1:\n",
    "    \n",
    "                            # Create pairs of consecutive NUTS_ID\n",
    "                            filtered_data['Next_NUTS_ID'] = filtered_data['NUTS_ID'].shift(-1)\n",
    "                            pairs = filtered_data[['NUTS_ID', 'Next_NUTS_ID']].dropna().apply(frozenset, axis=1)\n",
    "        \n",
    "                            # Filter pairs to only include those with home_NUTS_ID\n",
    "                            filtered_pairs = pairs[pairs.apply(lambda x: home_NUTS_ID in x)]\n",
    "        \n",
    "                            # Count each unique pair\n",
    "                            pair_counts = Counter(filtered_pairs)\n",
    "                            \n",
    "                            # Total count of pairs\n",
    "                            total_pairs = sum(pair_counts.values())\n",
    "        \n",
    "                            if total_pairs > 0:\n",
    "                            \n",
    "                                # Find the pair with the highest count\n",
    "                                most_common_pair, most_common_count = pair_counts.most_common(1)[0]\n",
    "                                \n",
    "                                # Calculate the ratio\n",
    "                                ratio = most_common_count / total_pairs          \n",
    "                                \n",
    "                                # Check if the ratio of the most common pair is more than 0.5\n",
    "                                if ratio > 0.5:\n",
    "                                    # Extract the other NUTS ID in the pair besides home_NUTS_ID\n",
    "                                    other_NUTS_ID = next(iter(most_common_pair - frozenset([home_NUTS_ID])))\n",
    "\n",
    "                                    # Perform lookup\n",
    "                                    pair_key = frozenset([home_NUTS_ID, other_NUTS_ID])\n",
    "                                    if pair_key in lookup_dict:\n",
    "                                        # Use period from the outer loop, home_NUTS_ID and other_NUTS_ID for the triple\n",
    "                                        LD_trips_user.add((period, home_NUTS_ID, other_NUTS_ID))\n",
    "\n",
    "                                else:\n",
    "                                    # If the ratio does not exceed 0.5, add pairs from filtered_pairs to multilocal_trips_user\n",
    "                                    for pair in filtered_pairs:\n",
    "                                        # Ensure the pair includes home_NUTS_ID and extract the other NUTS_ID\n",
    "                                        other_NUTS_ID = next(iter(pair - frozenset([home_NUTS_ID])))\n",
    "                                        # Perform lookup\n",
    "                                        pair_key = frozenset([home_NUTS_ID, other_NUTS_ID])\n",
    "                                        if pair_key in lookup_dict:\n",
    "                                            # Use period from the outer loop, home_NUTS_ID and other_NUTS_ID for the triple\n",
    "                                            multilocal_trips_user.add((period, home_NUTS_ID, other_NUTS_ID))\n",
    "                                        \n",
    "\n",
    "            \n",
    "            '''\n",
    "            NOMAD MOBILITY TYPE\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            # Convert nuts_dwell_time to a more accessible structure\n",
    "            dwell_time_dict = {}\n",
    "            for year, id, time_ in nuts_dwell_time:\n",
    "                if id not in dwell_time_dict:\n",
    "                    dwell_time_dict[id] = {}\n",
    "                dwell_time_dict[id][year] = time_\n",
    "            \n",
    "            nomad_ids = []\n",
    "            for id, years_counts in window_counts.items():\n",
    "                years = list(years_counts.keys())\n",
    "                # Check for IDs with exactly one occurrence\n",
    "                if any(count == 1 for count in years_counts.values()):\n",
    "                    nomad_ids.append(id)\n",
    "                \n",
    "                # Check for IDs with exactly two occurrences in consecutive years and total time <= 180\n",
    "                elif sum(years_counts.values()) == 2 and len(years)>1:\n",
    "                    if abs(years[0] - years[1]) == 1:  # Consecutive years\n",
    "                        total_time = sum(dwell_time_dict[id].get(year, 0) for year in years)\n",
    "                        if total_time <= 365:\n",
    "                            nomad_ids.append(id)\n",
    "                            \n",
    "            # Initialize an empty list to store the triples\n",
    "            nomad_trips_user = set()           \n",
    "            \n",
    "            # 3 Significant locations needed at least\n",
    "            if len(nomad_ids) > 2:\n",
    "\n",
    "                \n",
    "                \n",
    "                # Iterate through each period\n",
    "                for period, ids_in_period in periods.items():\n",
    "        \n",
    "                    # Filter IDs for this period\n",
    "                    filtered_ids = user_data[(user_data['NUTS_ID'].isin(nomad_ids)) & (user_data['Period'] == period)]\n",
    "                    \n",
    "                    # Drop duplicates and sort\n",
    "                    filtered_ids = filtered_ids.drop_duplicates(subset=['NUTS_ID'], keep='first')\n",
    "                    \n",
    "                    # Extract sorted NUTS_IDs\n",
    "                    sorted_ids = filtered_ids['NUTS_ID'].tolist()\n",
    "        \n",
    "                    if len(sorted_ids) > 1:\n",
    "                        \n",
    "                        # Create triples by iterating through sorted_ids and pairing each ID with the next one\n",
    "                        for i in range(len(sorted_ids) - 1):\n",
    "                            start_id = sorted_ids[i]\n",
    "                            end_id = sorted_ids[i + 1]\n",
    "                            \n",
    "                            # Perform lookup\n",
    "                            pair_key = frozenset([start_id, end_id])\n",
    "                            if pair_key in lookup_dict:\n",
    "                                # Use period from the outer loop, start_id and end_id for the triple\n",
    "                                nomad_trips_user.add((period, start_id, end_id))\n",
    "    \n",
    "    \n",
    "            '''\n",
    "            Cross Border\n",
    "        \n",
    "            '''\n",
    "    \n",
    "            CB_ids = set() # Cross Border\n",
    "            for _, ids in sig_locations_cross:\n",
    "                for id in ids:\n",
    "                    CB_ids.add(id)\n",
    "            CB_ids = list(CB_ids)\n",
    "\n",
    "            # Initialize an empty list to store the triples\n",
    "            CB_trips_user = set()\n",
    "\n",
    "            # Iterate through the periods and their corresponding IDs\n",
    "            for period, ids_in_period in periods_cross.items():\n",
    "                # Find the intersection of multilocal_ids and the current period's IDs\n",
    "                relevant_ids = set(CB_ids).intersection(ids_in_period)\n",
    "                \n",
    "                # Proceed if we have more than one ID in the intersection, thus capable of forming a pair\n",
    "                if len(relevant_ids) > 1:\n",
    "\n",
    "                    home_id_series = highest_value_id_per_period.loc[highest_value_id_per_period['Period'] == period, 'ID']\n",
    "\n",
    "                    if not home_id_series.empty:\n",
    "                        home_NUTS_ID = home_id_series.iloc[0]\n",
    "    \n",
    "                        # Filter user_data\n",
    "                        filtered_data = user_data[(user_data['NUTS_ID'].isin(relevant_ids)) & (user_data['Period'] == period)]\n",
    "                        # Create an independent copy of filtered_data to avoid SettingWithCopyWarning\n",
    "                        filtered_data = filtered_data.copy()\n",
    "                        filtered_data['Prev_NUTS_ID'] = filtered_data['NUTS_ID'].shift(1)\n",
    "                        filtered_data = filtered_data[filtered_data['NUTS_ID'] != filtered_data['Prev_NUTS_ID']].drop(columns='Prev_NUTS_ID')\n",
    "\n",
    "                        if len(filtered_data) > 1:\n",
    "    \n",
    "                            # Create pairs of consecutive NUTS_ID\n",
    "                            filtered_data['Next_NUTS_ID'] = filtered_data['NUTS_ID'].shift(-1)\n",
    "                            pairs = filtered_data[['NUTS_ID', 'Next_NUTS_ID']].dropna().apply(frozenset, axis=1)\n",
    "        \n",
    "                            # Filter pairs to only include those with home_NUTS_ID\n",
    "                            filtered_pairs = pairs[pairs.apply(lambda x: home_NUTS_ID in x)]\n",
    "        \n",
    "                            for pair in filtered_pairs:\n",
    "                                # Extract the other NUTS_ID\n",
    "                                other_NUTS_ID = next(iter(pair - frozenset([home_NUTS_ID])))\n",
    "                                if home_NUTS_ID[:2] != other_NUTS_ID[:2]: # If they are from different countries\n",
    "                                    # Perform lookup\n",
    "                                    pair_key = frozenset([home_NUTS_ID, other_NUTS_ID])\n",
    "                                    if pair_key in lookup_dict_cross:\n",
    "                                        # Use period from the outer loop, home_NUTS_ID and other_NUTS_ID for the triple\n",
    "                                        CB_trips_user.add((period, home_NUTS_ID, other_NUTS_ID))\n",
    "    \n",
    "                                    \n",
    "            \n",
    "    \n",
    "            '''\n",
    "            Finally return the results\n",
    "            '''\n",
    "\n",
    "            return multilocal_trips_user, nomad_trips_user, CB_trips_user, LD_trips_user\n",
    "\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")  # This will print the error message\n",
    "        print(user_id_of_interest)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf9fd3b9-850d-42dd-86d0-6397b547fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply updates in bulk\n",
    "def apply_updates_to_dataframes(updates_list):\n",
    "    # Global DataFrames\n",
    "    global multilocal_trips, nomad_trips, CB_trips, LD_trips\n",
    "    \n",
    "    for user_updates in updates_list:\n",
    "        multilocal_trips_user, nomad_trips_user, CB_trips_user, LD_trips_user = user_updates\n",
    "    \n",
    "        # Iterate over the triples\n",
    "        for year, nuts_id1, nuts_id2 in multilocal_trips_user:\n",
    "            # Check if this combination exists in the DataFrame\n",
    "            match = multilocal_trips[(multilocal_trips['NUTS_ID1'] == nuts_id1) & \n",
    "                                   (multilocal_trips['NUTS_ID2'] == nuts_id2) & \n",
    "                                   (multilocal_trips['Period'] == year)]\n",
    "            \n",
    "            if not match.empty:\n",
    "                # If exists, increment the 'Count'\n",
    "                multilocal_trips.loc[match.index, 'Count'] += 1\n",
    "            else:\n",
    "                # If not exists, create a new DataFrame for the row and concatenate it\n",
    "                new_row_df = pd.DataFrame({'NUTS_ID1': [nuts_id1], 'NUTS_ID2': [nuts_id2], 'Period': [year], 'Count': [1]})\n",
    "                multilocal_trips = pd.concat([multilocal_trips, new_row_df], ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # Iterate over the triples \n",
    "        for year, nuts_id1, nuts_id2 in nomad_trips_user:\n",
    "            # Check if this combination exists in the DataFrame\n",
    "            match = nomad_trips[(nomad_trips['NUTS_ID1'] == nuts_id1) & \n",
    "                                   (nomad_trips['NUTS_ID2'] == nuts_id2) & \n",
    "                                   (nomad_trips['Period'] == year)]\n",
    "            \n",
    "            if not match.empty:\n",
    "                # If exists, increment the 'Count'\n",
    "                nomad_trips.loc[match.index, 'Count'] += 1\n",
    "            else:\n",
    "                # If not exists, create a new DataFrame for the row and concatenate it\n",
    "                new_row_df = pd.DataFrame({'NUTS_ID1': [nuts_id1], 'NUTS_ID2': [nuts_id2], 'Period': [year], 'Count': [1]})\n",
    "                nomad_trips = pd.concat([nomad_trips, new_row_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "        # Iterate over the triples \n",
    "        for year, nuts_id1, nuts_id2 in CB_trips_user:\n",
    "            # Check if this combination exists in the DataFrame\n",
    "            match = CB_trips[(CB_trips['NUTS_ID1'] == nuts_id1) & \n",
    "                                   (CB_trips['NUTS_ID2'] == nuts_id2) & \n",
    "                                   (CB_trips['Period'] == year)]\n",
    "            \n",
    "            if not match.empty:\n",
    "                # If exists, increment the 'Count'\n",
    "                CB_trips.loc[match.index, 'Count'] += 1\n",
    "            else:\n",
    "                # If not exists, create a new DataFrame for the row and concatenate it\n",
    "                new_row_df = pd.DataFrame({'NUTS_ID1': [nuts_id1], 'NUTS_ID2': [nuts_id2], 'Period': [year], 'Count': [1]})\n",
    "                CB_trips = pd.concat([CB_trips, new_row_df], ignore_index=True)\n",
    "\n",
    "        \n",
    "        # Iterate over the triples \n",
    "        for year, nuts_id1, nuts_id2 in LD_trips_user:\n",
    "            # Check if this combination exists in the DataFrame\n",
    "            match = LD_trips[(LD_trips['NUTS_ID1'] == nuts_id1) & \n",
    "                                   (LD_trips['NUTS_ID2'] == nuts_id2) & \n",
    "                                   (LD_trips['Period'] == year)]\n",
    "            \n",
    "            if not match.empty:\n",
    "                # If exists, increment the 'Count'\n",
    "                LD_trips.loc[match.index, 'Count'] += 1\n",
    "            else:\n",
    "                # If not exists, create a new DataFrame for the row and concatenate it\n",
    "                new_row_df = pd.DataFrame({'NUTS_ID1': [nuts_id1], 'NUTS_ID2': [nuts_id2], 'Period': [year], 'Count': [1]})\n",
    "                LD_trips = pd.concat([LD_trips, new_row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6404da-149a-402e-93f1-d0ef0108bc72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 2024-04-06 17:31:36.076416\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Start time is {datetime.now()}\")\n",
    "\n",
    "num_workers = 8\n",
    "\n",
    "# Assuming these are your global DataFrames initialized somewhere above\n",
    "multilocal_trips = pd.DataFrame(columns=['NUTS_ID1', 'NUTS_ID2', 'Period', 'Count'])\n",
    "nomad_trips = pd.DataFrame(columns=['NUTS_ID1', 'NUTS_ID2', 'Period', 'Count'])\n",
    "CB_trips = pd.DataFrame(columns=['NUTS_ID1', 'NUTS_ID2', 'Period', 'Count'])\n",
    "LD_trips = pd.DataFrame(columns=['NUTS_ID1', 'NUTS_ID2', 'Period', 'Count'])\n",
    "\n",
    "# Placeholder for updates collected from all users\n",
    "all_user_updates = []\n",
    "\n",
    "# Use ProcessPoolExecutor to parallelize processing\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "\n",
    "    # Map process_user function to each user ID, and iterate over results as they become available\n",
    "    all_user_updates.extend(result for result in executor.map(process_user, unique_user_ids[:120000]) if result is not None)\n",
    "    \n",
    "# After collecting all updates, apply them to the global DataFrames\n",
    "apply_updates_to_dataframes(all_user_updates)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "print(f\"End time is {datetime.now()}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Calculate the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken to process the data: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5b87c15-f430-4851-b661-a8869654be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_updates_to_dataframes(all_user_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83424a88-7070-4962-a7f1-72d371cae423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.7776524267222"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59709.025122/37862*11000000/60/60/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "98630997-8c11-47bc-953c-54deb5c7da2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64800"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aebbb947-5314-4aab-9bce-17cc9d113cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.7026971290463"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end_time - start_time)/120000*11000000/60/60/24/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d6d337b2-2386-41e7-8c48-c37bcb343f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "77/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c8c54f28-1fa0-4455-b4ed-ed0660756b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUTS_ID1</th>\n",
       "      <th>NUTS_ID2</th>\n",
       "      <th>Period</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NUTS_ID1, NUTS_ID2, Period, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomad_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "de0ab7a4-8a89-4064-b026-3cb606530cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilocal_trips.Count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d1cf0a3b-b92f-473b-95ae-beb272d73353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_trips.Count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5d561677-21fa-4222-a4f2-ecb820a44f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_updates_to_dataframes(all_user_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869578a7-b15a-4192-ab6a-744d711a8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilocal_trips.to_csv('multilocal_trips_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0eb102-2f84-49d4-913b-bef4562fa820",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomad_trips.to_csv('nomad_trips_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb4a15-9382-4ac4-960d-b26e7385dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_trips.to_csv('CB_trips_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6ffd2-c6ed-4196-8f99-1f2fe36f5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LD_trips.to_csv('LD_trips_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b01f6f3c-ecf4-47e2-bb04-133956cbf086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUTS_ID1</th>\n",
       "      <th>NUTS_ID2</th>\n",
       "      <th>Period</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NUTS_ID1, NUTS_ID2, Period, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomad_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bdb20ecb-8796-4856-aabe-14a06a91f1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUTS_ID1</th>\n",
       "      <th>NUTS_ID2</th>\n",
       "      <th>Period</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES300</td>\n",
       "      <td>ES523</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES300</td>\n",
       "      <td>ES709</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES120</td>\n",
       "      <td>ES111</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES120</td>\n",
       "      <td>ES300</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE501</td>\n",
       "      <td>DE712</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ES705</td>\n",
       "      <td>ES220</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ES130</td>\n",
       "      <td>ES220</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ES705</td>\n",
       "      <td>ES213</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ES130</td>\n",
       "      <td>ES213</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>UKI74</td>\n",
       "      <td>UKK30</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NUTS_ID1 NUTS_ID2        Period Count\n",
       "0     ES300    ES523   before_2016     1\n",
       "1     ES300    ES709   before_2016     1\n",
       "2     ES120    ES111  2016_to_2019     1\n",
       "3     ES120    ES300  2016_to_2019     1\n",
       "4     DE501    DE712  2016_to_2019     1\n",
       "..      ...      ...           ...   ...\n",
       "84    ES705    ES220  2016_to_2019     1\n",
       "85    ES130    ES220   before_2016     1\n",
       "86    ES705    ES213  2016_to_2019     1\n",
       "87    ES130    ES213   before_2016     1\n",
       "88    UKI74    UKK30   before_2016     1\n",
       "\n",
       "[89 rows x 4 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilocal_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "41af23a8-6dde-4ca8-b315-ad482b3d3fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUTS_ID1</th>\n",
       "      <th>NUTS_ID2</th>\n",
       "      <th>Period</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NUTS_ID1, NUTS_ID2, Period, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5334.927958250046/10000*11000000/60/60/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8d7da864-a8d3-4ca7-b4d0-8d272da72685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUTS_ID1</th>\n",
       "      <th>NUTS_ID2</th>\n",
       "      <th>Period</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES300</td>\n",
       "      <td>UKI71</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE501</td>\n",
       "      <td>ES512</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DED41</td>\n",
       "      <td>ITF64</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE128</td>\n",
       "      <td>PL524</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FR101</td>\n",
       "      <td>ES300</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>AT334</td>\n",
       "      <td>DE714</td>\n",
       "      <td>2020_and_beyond</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>TR321</td>\n",
       "      <td>ITG17</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>TR100</td>\n",
       "      <td>UKI45</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>TR823</td>\n",
       "      <td>LT022</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>DEA11</td>\n",
       "      <td>ES213</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NUTS_ID1 NUTS_ID2           Period Count\n",
       "0      ES300    UKI71      before_2016     1\n",
       "1      DE501    ES512      before_2016     1\n",
       "2      DED41    ITF64      before_2016     1\n",
       "3      DE128    PL524      before_2016     1\n",
       "4      FR101    ES300      before_2016     2\n",
       "..       ...      ...              ...   ...\n",
       "648    AT334    DE714  2020_and_beyond     1\n",
       "649    TR321    ITG17      before_2016     1\n",
       "650    TR100    UKI45      before_2016     1\n",
       "651    TR823    LT022      before_2016     1\n",
       "652    DEA11    ES213      before_2016     1\n",
       "\n",
       "[653 rows x 4 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CB_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "11bb5643-fdee-4063-86c6-849b638e58e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUTS_ID1</th>\n",
       "      <th>NUTS_ID2</th>\n",
       "      <th>Period</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES300</td>\n",
       "      <td>UKI71</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE501</td>\n",
       "      <td>ES512</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES612</td>\n",
       "      <td>ES300</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DED41</td>\n",
       "      <td>ITF64</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SE121</td>\n",
       "      <td>SE313</td>\n",
       "      <td>before_2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>TR611</td>\n",
       "      <td>TR221</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>FRL04</td>\n",
       "      <td>FRI32</td>\n",
       "      <td>2020_and_beyond</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>UKE42</td>\n",
       "      <td>UKL22</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>FR101</td>\n",
       "      <td>FRI32</td>\n",
       "      <td>2020_and_beyond</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>UKK30</td>\n",
       "      <td>UKI32</td>\n",
       "      <td>2016_to_2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NUTS_ID1 NUTS_ID2           Period Count\n",
       "0       ES300    UKI71      before_2016     1\n",
       "1       DE501    ES512      before_2016     1\n",
       "2       ES612    ES300      before_2016     3\n",
       "3       DED41    ITF64      before_2016     1\n",
       "4       SE121    SE313      before_2016     1\n",
       "...       ...      ...              ...   ...\n",
       "2259    TR611    TR221     2016_to_2019     1\n",
       "2260    FRL04    FRI32  2020_and_beyond     1\n",
       "2261    UKE42    UKL22     2016_to_2019     1\n",
       "2262    FR101    FRI32  2020_and_beyond     1\n",
       "2263    UKK30    UKI32     2016_to_2019     1\n",
       "\n",
       "[2264 rows x 4 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LD_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7e989808-d6bf-4c35-8033-cc1ec4693b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.27697731388939"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5334.927958250046/10000*400000/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c200514-37de-46ca-a2bc-48e2714c73cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
